import pandas as pd
import numpy as np
import re
import jieba
import joblib
import os
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import sys
sys.path.append('../')
from src.text_preprocessor import TextPreprocessor

# ğŸ“‹ 1. åŠ è½½æ•°æ®æ–‡ä»¶
file_path = '../data/raw/è¯„è®ºå’Œæ­£æ–‡.xlsx'
print(f"åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
df = pd.read_excel(file_path)

# æ£€æŸ¥æ˜¯å¦åŒ…å« 'è¯„è®ºå†…å®¹' åˆ—
if 'è¯„è®ºå†…å®¹' not in df.columns:
    raise ValueError("æ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘ 'è¯„è®ºå†…å®¹' åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ã€‚")

print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")

# ğŸ“‹ 2. æ–‡æœ¬é¢„å¤„ç†ï¼ˆä½¿ç”¨ä¸ç”¨æˆ·ç›¸åŒçš„TextPreprocessorï¼‰
print("å¼€å§‹æ–‡æœ¬é¢„å¤„ç†...")
preprocessor = TextPreprocessor()

def preprocess_text(text):
    if pd.isnull(text):
        return ""
    # ä½¿ç”¨ä¸ç”¨æˆ·ç›¸åŒçš„é¢„å¤„ç†æµç¨‹
    processed_words = preprocessor.preprocess(text)
    return " ".join(processed_words)

# åº”ç”¨æ–‡æœ¬é¢„å¤„ç†
df['cleaned_text'] = df['è¯„è®ºå†…å®¹'].apply(preprocess_text)

# è¿‡æ»¤æ‰ç©ºçš„æ–‡æœ¬
df = df[df['cleaned_text'].str.len() > 0]
print(f"é¢„å¤„ç†å®Œæˆï¼Œè¿‡æ»¤åå‰©ä½™ {len(df)} æ¡æœ‰æ•ˆè®°å½•")

# ğŸ“‹ 3. æ•°æ®åˆ’åˆ† - é¦–å…ˆè¿›è¡Œæ•°æ®åˆ’åˆ†ï¼Œç¡®ä¿æµ‹è¯•é›†å®Œå…¨éš”ç¦»
print("å¼€å§‹æ•°æ®åˆ’åˆ†...")
# ç¬¬ä¸€æ­¥ï¼šå°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ80%ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ20%ï¼‰ï¼Œæµ‹è¯•é›†å®Œå…¨éš”ç¦»
# æš‚æ—¶ä¸ä½¿ç”¨stratifyï¼Œå› ä¸ºdominant_topicè¿˜æœªç”Ÿæˆ
df_train_val, df_test = train_test_split(
    df, test_size=0.2, random_state=42
)

# ç¬¬äºŒæ­¥ï¼šåœ¨è®­ç»ƒéªŒè¯é›†ä¸­å†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ75%ï¼‰å’ŒéªŒè¯é›†ï¼ˆ25%ï¼‰
df_train, df_val = train_test_split(
    df_train_val, test_size=0.25, random_state=42
)

print(f"æ•°æ®åˆ’åˆ†å®Œæˆ:")
print(f"- è®­ç»ƒé›†: {len(df_train)} æ¡è®°å½•")
print(f"- éªŒè¯é›†: {len(df_val)} æ¡è®°å½•")
print(f"- æµ‹è¯•é›†: {len(df_test)} æ¡è®°å½•")

# ğŸ“‹ 4. å‘é‡åŒ–æ–‡æœ¬æ•°æ®ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
print("å¼€å§‹å‘é‡åŒ–æ–‡æœ¬æ•°æ®ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰...")
# ä½¿ç”¨æ›´åˆç†çš„å‚æ•°
vectorizer = CountVectorizer(
    max_features=8000,      # å¢åŠ ç‰¹å¾æ•°é‡
    min_df=2,               # è‡³å°‘åœ¨2ä¸ªæ–‡æ¡£ä¸­å‡ºç°
    max_df=0.9,             # æœ€å¤šåœ¨90%çš„æ–‡æ¡£ä¸­å‡ºç°
    ngram_range=(1, 2)      # åŒ…å«1-gramå’Œ2-gram
)
X_train_count = vectorizer.fit_transform(df_train['cleaned_text'])
print(f"å‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {X_train_count.shape[1]}")

# è‡ªåŠ¨ç”Ÿæˆä¸»é¢˜ä¸šåŠ¡åŒ–æ ‡ç­¾
def get_business_labels(n_topics, lda_model=None, vectorizer=None):
    """
    åŸºäºå…³é”®è¯è¯­ä¹‰è‡ªåŠ¨ç”Ÿæˆä¸šåŠ¡åŒ–æ ‡ç­¾
    
    Args:
        n_topics: ä¸»é¢˜æ•°é‡
        lda_model: LDAæ¨¡å‹ï¼ˆç”¨äºæå–å…³é”®è¯ï¼‰
        vectorizer: å‘é‡åŒ–å™¨ï¼ˆç”¨äºè·å–ç‰¹å¾åç§°ï¼‰
        
    Returns:
        ä¸šåŠ¡æ ‡ç­¾å­—å…¸ï¼Œæ ¼å¼ï¼š{ä¸»é¢˜ç¼–å·: ä¸šåŠ¡æ ‡ç­¾}
    """
    business_labels = {}
    
    # å¦‚æœæä¾›äº†æ¨¡å‹å’Œå‘é‡åŒ–å™¨ï¼ŒåŸºäºå…³é”®è¯è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾
    if lda_model and vectorizer:
        feature_names = vectorizer.get_feature_names_out()
        for topic_idx in range(n_topics):
            # è·å–ä¸»é¢˜çš„å‰10ä¸ªå…³é”®è¯
            topic = lda_model.components_[topic_idx]
            top_words = [feature_names[i] for i in topic.argsort()[:-10:-1]]
            
            # åŸºäºå…³é”®è¯è¯­ä¹‰ç”Ÿæˆæ ‡ç­¾
            # 1. æå–æ ¸å¿ƒæ¦‚å¿µ
            core_concepts = []
            
            # ç¤¾äº¤ç›¸å…³å…³é”®è¯
            social_words = ['ç¤¾äº¤', 'çº¿ä¸‹', 'åœˆå­', 'ç¤¾åŒº', 'äº’åŠ¨', 'äº¤æµ']
            if any(word in top_words for word in social_words):
                core_concepts.append('ç¤¾äº¤ä½“éªŒ')
            
            # åŒ¹é…ç›¸å…³å…³é”®è¯
            match_words = ['åŒ¹é…', 'æ¨è', 'ç»“æœ', 'è´¨é‡', 'ç²¾å‡†']
            if any(word in top_words for word in match_words):
                core_concepts.append('åŒ¹é…æ•ˆæœ')
            
            # è„±å•ç›¸å…³å…³é”®è¯
            dating_words = ['è„±å•', 'æ‹çˆ±', 'ç”·æœ‹å‹', 'å¥³æœ‹å‹', 'ç»“å©š']
            if any(word in top_words for word in dating_words):
                core_concepts.append('è„±å•æ•ˆæœ')
            
            # ç”¨æˆ·ç›¸å…³å…³é”®è¯
            user_words = ['ç”¨æˆ·', 'è´¨é‡', 'æ­£å¸¸äºº', 'çœŸå®', 'é è°±']
            if any(word in top_words for word in user_words):
                core_concepts.append('ç”¨æˆ·è´¨é‡')
            
            # åŠŸèƒ½ç›¸å…³å…³é”®è¯
            feature_words = ['åŠŸèƒ½', 'app', 'AI', 'æ¢æ¢', 'æŠ–éŸ³']
            if any(word in top_words for word in feature_words):
                core_concepts.append('åŠŸèƒ½è¯„ä»·')
            
            # ä½“éªŒç›¸å…³å…³é”®è¯
            experience_words = ['ä½“éªŒ', 'å¥½ç”¨', 'æ»¡æ„', 'å¸è½½', 'å¤±æœ›']
            if any(word in top_words for word in experience_words):
                core_concepts.append('ä½¿ç”¨ä½“éªŒ')
            
            # 2. ç”Ÿæˆæ ‡ç­¾
            if core_concepts:
                # ç»„åˆæ ¸å¿ƒæ¦‚å¿µ
                label = 'ä¸'.join(core_concepts[:2])  # æœ€å¤šå–2ä¸ªæ ¸å¿ƒæ¦‚å¿µ
                # æ·»åŠ è¯„ä»·ç»´åº¦
                evaluation_words = ['è¯„ä»·', 'å¯¹æ¯”', 'ç•™å­˜', 'æ»¡æ„åº¦', 'æœåŠ¡']
                for word in evaluation_words:
                    if word in top_words:
                        label += f'ä¸{word}'
                        break
                # ç¡®ä¿æ ‡ç­¾é•¿åº¦åˆç†
                if len(label) < 5:
                    label += 'åˆ†æ'
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ¸å¿ƒæ¦‚å¿µï¼Œä½¿ç”¨é€šç”¨æ ‡ç­¾
                label = f'ä¸»é¢˜{topic_idx+1}åˆ†æ'
            
            business_labels[topic_idx] = label
    else:
        #  fallback: ä½¿ç”¨é€šç”¨æ ‡ç­¾
        for i in range(n_topics):
            business_labels[i] = f'ä¸»é¢˜{i+1}åˆ†æ'
        
    return business_labels

# ğŸ“‹ 5. è‡ªåŠ¨è¯„ä¼°å¹¶é€‰æ‹©æœ€ä½³ä¸»é¢˜æ•°
print("å¼€å§‹è‡ªåŠ¨è¯„ä¼°æœ€ä½³ä¸»é¢˜æ•°...")

def evaluate_topic_numbers(X_train, topic_numbers=[4, 5, 6], max_iter=200):
    """
    è¯„ä¼°ä¸åŒä¸»é¢˜æ•°çš„æ€§èƒ½ï¼Œé€‰æ‹©æœ€ä½³ä¸»é¢˜æ•°
    
    Args:
        X_train: è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µ
        topic_numbers: å¾…è¯„ä¼°çš„ä¸»é¢˜æ•°åˆ—è¡¨
        max_iter: è¿­ä»£æ¬¡æ•°
        
    Returns:
        best_n_topics: æœ€ä½³ä¸»é¢˜æ•°
        best_perplexity: æœ€ä½å›°æƒ‘åº¦
        evaluation_results: æ‰€æœ‰è¯„ä¼°ç»“æœ
    """
    evaluation_results = []
    
    for n_topics in topic_numbers:
        print(f"è¯„ä¼°ä¸»é¢˜æ•°: {n_topics}...")
        
        # è®­ç»ƒLDAæ¨¡å‹
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            max_iter=max_iter,
            learning_method='online',
            learning_offset=10.0,
            doc_topic_prior=0.1,
            topic_word_prior=0.01
        )
        lda.fit(X_train)
        
        # è®¡ç®—å›°æƒ‘åº¦
        perplexity = lda.perplexity(X_train)
        
        # è®¡ç®—ä¸»é¢˜æ¸…æ™°åº¦ï¼ˆä¸»é¢˜è¯åˆ†å¸ƒçš„ç†µï¼‰
        topic_entropy = []
        feature_names = vectorizer.get_feature_names_out()
        for topic in lda.components_:
            topic_dist = topic / topic.sum()
            entropy = -np.sum(topic_dist * np.log(topic_dist + 1e-10))
            topic_entropy.append(entropy)
        avg_topic_clarity = 1 - (np.mean(topic_entropy) / np.log(len(feature_names)))
        
        evaluation_results.append({
            'n_topics': n_topics,
            'perplexity': perplexity,
            'avg_topic_clarity': avg_topic_clarity
        })
        
        print(f"  å›°æƒ‘åº¦: {perplexity:.2f}, ä¸»é¢˜æ¸…æ™°åº¦: {avg_topic_clarity:.4f}")
    
    # é€‰æ‹©æœ€ä½³ä¸»é¢˜æ•°ï¼ˆåŸºäºå›°æƒ‘åº¦ï¼‰
    best_result = min(evaluation_results, key=lambda x: x['perplexity'])
    best_n_topics = best_result['n_topics']
    best_perplexity = best_result['perplexity']
    
    print(f"\næœ€ä½³ä¸»é¢˜æ•°: {best_n_topics}")
    print(f"æœ€ä½å›°æƒ‘åº¦: {best_perplexity:.2f}")
    
    return best_n_topics, best_perplexity, evaluation_results

# è¯„ä¼°æœ€ä½³ä¸»é¢˜æ•°
topic_numbers_to_evaluate = [4, 5, 6, 7]  # æ‰©å±•è¯„ä¼°èŒƒå›´
n_topics, best_perplexity, evaluation_results = evaluate_topic_numbers(X_train_count, topic_numbers_to_evaluate)

# è·å–ä¸šåŠ¡æ ‡ç­¾ï¼ˆåœ¨LDAæ¨¡å‹è®­ç»ƒå®Œæˆåè°ƒç”¨ï¼‰
# æ³¨æ„ï¼šè¿™é‡Œå…ˆè®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼ï¼Œåç»­ä¼šåœ¨LDAæ¨¡å‹è®­ç»ƒå®Œæˆåæ›´æ–°
business_labels = get_business_labels(n_topics)
# è®­ç»ƒæœ€ç»ˆLDAæ¨¡å‹
print(f"\nå¼€å§‹è®­ç»ƒæœ€ç»ˆLDAæ¨¡å‹ï¼ˆä¸»é¢˜æ•°={n_topics}ï¼‰...")
lda = LatentDirichletAllocation(
    n_components=n_topics,
    random_state=42,
    max_iter=200,
    learning_method='online',
    learning_offset=10.0,
    doc_topic_prior=0.1,
    topic_word_prior=0.01
)
lda.fit(X_train_count)
print("LDAæ¨¡å‹è®­ç»ƒå®Œæˆ")

# åŸºäºè®­ç»ƒå¥½çš„LDAæ¨¡å‹è‡ªåŠ¨ç”Ÿæˆä¸šåŠ¡æ ‡ç­¾
print("åŸºäºå…³é”®è¯è‡ªåŠ¨ç”Ÿæˆä¸šåŠ¡æ ‡ç­¾...")
business_labels = get_business_labels(n_topics, lda, vectorizer)
print("ä¸šåŠ¡æ ‡ç­¾ç”Ÿæˆå®Œæˆ")
# ğŸ“‹ 6. è¾“å‡ºæ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
def get_lda_keywords(model, vectorizer, business_labels, n_top_words=15):
    keywords = {}
    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(model.components_):
        topic_label = business_labels.get(topic_idx, f"ä¸»é¢˜ {topic_idx+1}")
        keywords[topic_label] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
    return keywords

def get_tfidf_keywords_by_topic(df, tfidf_vectorizer, business_labels, n_top_words=10):
    """
    åŸºäºTF-IDFä¸ºæ¯ä¸ªä¸»é¢˜æå–å…³é”®è¯
    
    Args:
        df: åŒ…å«æ–‡æœ¬å’Œä¸»é¢˜çš„æ•°æ®æ¡†
        tfidf_vectorizer: TF-IDFå‘é‡åŒ–å™¨
        business_labels: ä¸šåŠ¡æ ‡ç­¾å­—å…¸
        n_top_words: æ¯ä¸ªä¸»é¢˜æå–çš„å…³é”®è¯æ•°é‡
        
    Returns:
        æ¯ä¸ªä¸»é¢˜çš„TF-IDFå…³é”®è¯å­—å…¸
    """
    topic_keywords = {}
    topics = df['dominant_topic'].unique()
    
    for topic in topics:
        # è·å–è¯¥ä¸»é¢˜çš„æ‰€æœ‰æ–‡æ¡£
        topic_docs = df[df['dominant_topic'] == topic]['cleaned_text']
        
        if len(topic_docs) > 0:
            # è®¡ç®—è¯¥ä¸»é¢˜æ‰€æœ‰æ–‡æ¡£çš„TF-IDF
            tfidf_matrix = tfidf_vectorizer.transform(topic_docs)
            # è®¡ç®—æ¯ä¸ªè¯çš„å¹³å‡TF-IDFå€¼
            avg_tfidf = tfidf_matrix.mean(axis=0).A1
            # è·å–ç‰¹å¾åç§°
            feature_names = tfidf_vectorizer.get_feature_names_out()
            # æŒ‰TF-IDFå€¼æ’åºï¼Œå–å‰n_top_wordsä¸ª
            top_indices = avg_tfidf.argsort()[-n_top_words:][::-1]
            top_words = [feature_names[i] for i in top_indices]
            topic_label = business_labels.get(topic, f"ä¸»é¢˜ {topic+1}")
            topic_keywords[topic_label] = top_words
    
    return topic_keywords

def get_combined_keywords(lda_keywords, tfidf_keywords, n_top_words=10):
    """
    ç»“åˆLDAå’ŒTF-IDFå…³é”®è¯ï¼Œç¡®ä¿ä¸¤è€…éƒ½æœ‰è´¡çŒ®
    
    Args:
        lda_keywords: LDAä¸»é¢˜å…³é”®è¯
        tfidf_keywords: TF-IDFä¸»é¢˜å…³é”®è¯
        n_top_words: æ¯ä¸ªä¸»é¢˜ä¿ç•™çš„å…³é”®è¯æ•°é‡
        
    Returns:
        æ¯ä¸ªä¸»é¢˜çš„ç»„åˆå…³é”®è¯å­—å…¸
    """
    combined_keywords = {}
    
    for topic in lda_keywords:
        if topic in tfidf_keywords:
            # åˆå¹¶ä¸¤ç§å…³é”®è¯ï¼Œå»é‡ï¼Œç¡®ä¿å¹³è¡¡èåˆ
            combined = []
            seen = set()
            
            # äº¤æ›¿æ·»åŠ LDAå’ŒTF-IDFå…³é”®è¯ï¼Œç¡®ä¿ä¸¤è€…éƒ½æœ‰è´¡çŒ®
            lda_words = lda_keywords[topic]
            tfidf_words = tfidf_keywords[topic]
            
            max_len = max(len(lda_words), len(tfidf_words))
            
            for i in range(max_len):
                # æ·»åŠ LDAå…³é”®è¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if i < len(lda_words):
                    word = lda_words[i]
                    if word not in seen:
                        combined.append(word)
                        seen.add(word)
                        if len(combined) >= n_top_words:
                            break
                
                # æ·»åŠ TF-IDFå…³é”®è¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if i < len(tfidf_words):
                    word = tfidf_words[i]
                    if word not in seen:
                        combined.append(word)
                        seen.add(word)
                        if len(combined) >= n_top_words:
                            break
            
            # å¦‚æœè¿˜ä¸å¤Ÿï¼Œæ·»åŠ å‰©ä½™çš„LDAå…³é”®è¯
            if len(combined) < n_top_words:
                for word in lda_words:
                    if word not in seen:
                        combined.append(word)
                        seen.add(word)
                        if len(combined) >= n_top_words:
                            break
            
            # å¦‚æœè¿˜ä¸å¤Ÿï¼Œæ·»åŠ å‰©ä½™çš„TF-IDFå…³é”®è¯
            if len(combined) < n_top_words:
                for word in tfidf_words:
                    if word not in seen:
                        combined.append(word)
                        seen.add(word)
                        if len(combined) >= n_top_words:
                            break
            
            combined_keywords[topic] = combined
        else:
            combined_keywords[topic] = lda_keywords[topic][:n_top_words]
    
    return combined_keywords

# æå–å…³é”®è¯
keywords = get_lda_keywords(lda, vectorizer, business_labels)
print("\nè‡ªåŠ¨ç”Ÿæˆçš„ä¸»é¢˜å…³é”®è¯ï¼š")
for theme, words in keywords.items():
    print(f"{theme}: {', '.join(words[:10])}")

# ğŸ“‹ 7. ä¸ºè®­ç»ƒé›†åˆ†é…ä¸»é¢˜
print("ä¸ºè®­ç»ƒé›†åˆ†é…ä¸»é¢˜...")
doc_topic_dist_train = lda.transform(X_train_count)
df_train['dominant_topic'] = doc_topic_dist_train.argmax(axis=1)

# ğŸ“‹ 8. å‡†å¤‡TF-IDFç‰¹å¾ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
print("åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰...")
tfidf_vectorizer = TfidfVectorizer(
    max_features=8000,
    min_df=2,
    max_df=0.9,
    ngram_range=(1, 2)
)
X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['cleaned_text'])
print(f"TF-IDFå‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {X_train_tfidf.shape[1]}")

# ğŸ“‹ 9. æå–æ¯ä¸ªä¸»é¢˜çš„TF-IDFå…³é”®è¯
print("\næå–æ¯ä¸ªä¸»é¢˜çš„TF-IDFå…³é”®è¯ï¼š")
tfidf_keywords = get_tfidf_keywords_by_topic(df_train, tfidf_vectorizer, business_labels)
for theme, words in tfidf_keywords.items():
    print(f"{theme} (TF-IDF): {', '.join(words[:10])}")

# ğŸ“‹ 10. ç»“åˆLDAå’ŒTF-IDFå…³é”®è¯
print("\nç»“åˆLDAå’ŒTF-IDFå…³é”®è¯ï¼š")
combined_keywords = get_combined_keywords(keywords, tfidf_keywords)
for theme, words in combined_keywords.items():
    print(f"{theme} (ç»„åˆ): {', '.join(words[:10])}")

# ğŸ“‹ 9. ä¸ºè®­ç»ƒé›†åˆå¹¶ç‰¹å¾
print("ä¸ºè®­ç»ƒé›†åˆå¹¶ç‰¹å¾...")
X_combined_train = np.hstack([doc_topic_dist_train, X_train_tfidf.toarray()])
print(f"åˆå¹¶åç‰¹å¾ç»´åº¦: {X_combined_train.shape[1]}")

# ğŸ“‹ 10. ä¸ºéªŒè¯é›†æå–ç‰¹å¾ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„æ¨¡å‹ï¼‰
print("ä¸ºéªŒè¯é›†æå–ç‰¹å¾...")
X_val_count = vectorizer.transform(df_val['cleaned_text'])
X_val_tfidf = tfidf_vectorizer.transform(df_val['cleaned_text'])
doc_topic_dist_val = lda.transform(X_val_count)
X_combined_val = np.hstack([doc_topic_dist_val, X_val_tfidf.toarray()])
df_val['dominant_topic'] = doc_topic_dist_val.argmax(axis=1)

# ğŸ“‹ 11. ä¸ºæµ‹è¯•é›†æå–ç‰¹å¾ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„æ¨¡å‹ï¼‰
print("ä¸ºæµ‹è¯•é›†æå–ç‰¹å¾...")
X_test_count = vectorizer.transform(df_test['cleaned_text'])
X_test_tfidf = tfidf_vectorizer.transform(df_test['cleaned_text'])
doc_topic_dist_test = lda.transform(X_test_count)
X_combined_test = np.hstack([doc_topic_dist_test, X_test_tfidf.toarray()])
df_test['dominant_topic'] = doc_topic_dist_test.argmax(axis=1)

# ğŸ“‹ 12. è®­ç»ƒä¸»é¢˜åˆ†ç±»å™¨ï¼ˆæ”¹è¿›å‚æ•°ï¼‰
print("å¼€å§‹è®­ç»ƒä¸»é¢˜åˆ†ç±»å™¨...")
# ä½¿ç”¨æ”¹è¿›çš„RandomForestå‚æ•°
theme_classifier = RandomForestClassifier(
    n_estimators=100,        # å¢åŠ æ ‘çš„æ•°é‡
    max_depth=50,            # å¢åŠ æ ‘çš„æ·±åº¦
    min_samples_split=5,     # æœ€å°åˆ†è£‚æ ·æœ¬æ•°
    min_samples_leaf=2,      # æœ€å°å¶èŠ‚ç‚¹æ ·æœ¬æ•°
    random_state=42,
    n_jobs=-1                # ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒ
)
theme_classifier.fit(X_combined_train, df_train['dominant_topic'])

# ğŸ“‹ 13. è¯„ä¼°æ¨¡å‹
print("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
y_val_pred = theme_classifier.predict(X_combined_val)
val_accuracy = accuracy_score(df_val['dominant_topic'], y_val_pred)
print(f"\néªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ˆæœ€ç»ˆè¯„ä¼°ï¼‰
y_test_pred = theme_classifier.predict(X_combined_test)
test_accuracy = accuracy_score(df_test['dominant_topic'], y_test_pred)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")

# æ‰“å°è¯¦ç»†è¯„ä¼°æŠ¥å‘Šï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰
print("\nåˆ†ç±»æŠ¥å‘Šï¼ˆæµ‹è¯•é›†ï¼‰:")
target_names = [business_labels.get(i, f"ä¸»é¢˜ {i+1}") for i in range(n_topics)]
print(classification_report(df_test['dominant_topic'], y_test_pred, target_names=target_names))

# è®¡ç®—å›°æƒ‘åº¦ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
perplexity = lda.perplexity(X_train_count)
print(f"\nLDAæ¨¡å‹å›°æƒ‘åº¦: {perplexity:.2f}")



# ğŸ“‹ 14. ä¿å­˜æ¨¡å‹å’Œå‘é‡åŒ–å™¨
model_dir = '../data/models'
os.makedirs(model_dir, exist_ok=True)

print(f"\nä¿å­˜æ¨¡å‹åˆ° {model_dir}...")
joblib.dump(lda, os.path.join(model_dir, 'lda_model.pkl'))
joblib.dump(vectorizer, os.path.join(model_dir, 'count_vectorizer.pkl'))
joblib.dump(tfidf_vectorizer, os.path.join(model_dir, 'vectorizer.pkl'))
joblib.dump(theme_classifier, os.path.join(model_dir, 'theme_classification_model.pkl'))
joblib.dump(keywords, os.path.join(model_dir, 'theme_keywords.pkl'))

print("\næ‰€æœ‰æ¨¡å‹å·²ä¿å­˜å®Œæˆï¼")
print(f"- LDAæ¨¡å‹: lda_model.pkl")
print(f"- Countå‘é‡åŒ–å™¨: count_vectorizer.pkl")
print(f"- TF-IDFå‘é‡åŒ–å™¨: vectorizer.pkl")
print(f"- ä¸»é¢˜åˆ†ç±»å™¨: theme_classification_model.pkl")
print(f"- ä¸»é¢˜å…³é”®è¯: theme_keywords.pkl")

# ğŸ“‹ 14. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
print("\nè®­ç»ƒå®Œæˆï¼Œç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
# åˆ›å»ºlogsæ–‡ä»¶å¤¹
logs_dir = 'logs'
os.makedirs(logs_dir, exist_ok=True)

# åˆ›å»ºä¸»é¢˜ç¼–å·åˆ°ä¸šåŠ¡æ ‡ç­¾çš„æ˜ å°„
topic_to_label = business_labels

# åˆ›å»ºä¸šåŠ¡æ ‡ç­¾åˆ°ä¸»é¢˜ç¼–å·çš„åå‘æ˜ å°„
label_to_topic = {label: topic for topic, label in business_labels.items()}

with open(os.path.join(logs_dir, 'train_log_new.txt'), 'w', encoding='utf-8') as f:
    f.write("=== æ¨¡å‹è®­ç»ƒæŠ¥å‘Š ===\n")
    f.write(f"è®­ç»ƒæ—¶é—´: {pd.Timestamp.now()}\n")
    f.write(f"æ€»æ•°æ®é‡: {len(df)}\n")
    f.write(f"è®­ç»ƒé›†: {len(df_train)} æ¡è®°å½•\n")
    f.write(f"éªŒè¯é›†: {len(df_val)} æ¡è®°å½•\n")
    f.write(f"æµ‹è¯•é›†: {len(df_test)} æ¡è®°å½•\n")
    f.write(f"ç‰¹å¾ç»´åº¦: {X_train_count.shape[1]}\n")
    f.write(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}\n")
    f.write(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}\n")
    f.write(f"LDAå›°æƒ‘åº¦: {perplexity:.2f}\n")
    f.write(f"æœ€ä½³ä¸»é¢˜æ•°: {n_topics}\n")
    f.write(f"ä¸»é¢˜æ•°è¯„ä¼°ç»“æœ:\n")
    for result in evaluation_results:
        f.write(f"  ä¸»é¢˜æ•°={result['n_topics']}: å›°æƒ‘åº¦={result['perplexity']:.2f}, ä¸»é¢˜æ¸…æ™°åº¦={result['avg_topic_clarity']:.4f}\n")
    f.write("\n")

    f.write("=== ä¸»é¢˜ä¸šåŠ¡æ ‡ç­¾ ===\n")
    for topic_id, label in business_labels.items():
        f.write(f"{label}\n")
    f.write("\n")

    f.write("=== LDAä¸»é¢˜å…³é”®è¯ ===\n")
    for topic_idx, topic in enumerate(lda.components_):
        topic_label = business_labels.get(topic_idx, f"ä¸»é¢˜ {topic_idx+1}")
        top_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
        f.write(f"{topic_label}: {', '.join(top_words)}\n")
    
    f.write("\n=== TF-IDFä¸»é¢˜å…³é”®è¯ ===\n")
    # ä¸ºæ¯ä¸ªä¸»é¢˜ç”ŸæˆTF-IDFå…³é”®è¯
    tfidf_keywords_by_label = {}
    for topic in df_train['dominant_topic'].unique():
        topic_docs = df_train[df_train['dominant_topic'] == topic]['cleaned_text']
        if len(topic_docs) > 0:
            tfidf_matrix = tfidf_vectorizer.transform(topic_docs)
            avg_tfidf = tfidf_matrix.mean(axis=0).A1
            feature_names = tfidf_vectorizer.get_feature_names_out()
            top_indices = avg_tfidf.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_indices]
            topic_label = business_labels.get(topic, f"ä¸»é¢˜ {topic+1}")
            tfidf_keywords_by_label[topic_label] = top_words
    
    for label, words in tfidf_keywords_by_label.items():
        f.write(f"{label}: {', '.join(words)}\n")
    
    f.write("\n=== ç»„åˆä¸»é¢˜å…³é”®è¯ ===\n")
    # ç”Ÿæˆç»„åˆå…³é”®è¯
    combined_keywords_by_label = {}
    for topic_idx, topic in enumerate(lda.components_):
        topic_label = business_labels.get(topic_idx, f"ä¸»é¢˜ {topic_idx+1}")
        # è·å–LDAå…³é”®è¯
        lda_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-16:-1]]
        # è·å–TF-IDFå…³é”®è¯
        topic_docs = df_train[df_train['dominant_topic'] == topic_idx]['cleaned_text']
        tfidf_words = []
        if len(topic_docs) > 0:
            tfidf_matrix = tfidf_vectorizer.transform(topic_docs)
            avg_tfidf = tfidf_matrix.mean(axis=0).A1
            feature_names = tfidf_vectorizer.get_feature_names_out()
            top_indices = avg_tfidf.argsort()[-10:][::-1]
            tfidf_words = [feature_names[i] for i in top_indices]
        
        # ç»„åˆå…³é”®è¯
        combined = []
        seen = set()
        max_len = max(len(lda_words), len(tfidf_words))
        
        for i in range(max_len):
            if i < len(lda_words):
                word = lda_words[i]
                if word not in seen:
                    combined.append(word)
                    seen.add(word)
                    if len(combined) >= 10:
                        break
            if i < len(tfidf_words):
                word = tfidf_words[i]
                if word not in seen:
                    combined.append(word)
                    seen.add(word)
                    if len(combined) >= 10:
                        break
        
        if len(combined) < 10:
            for word in lda_words:
                if word not in seen:
                    combined.append(word)
                    seen.add(word)
                    if len(combined) >= 10:
                        break
        
        if len(combined) < 10:
            for word in tfidf_words:
                if word not in seen:
                    combined.append(word)
                    seen.add(word)
                    if len(combined) >= 10:
                        break
        
        combined_keywords_by_label[topic_label] = combined
    
    for label, words in combined_keywords_by_label.items():
        f.write(f"{label}: {', '.join(words)}\n")

print(f"è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {os.path.join(logs_dir, 'train_log_new.txt')}")

# æ‰“å°ä¸šåŠ¡æ ‡ç­¾
print("\nä¸»é¢˜ä¸šåŠ¡æ ‡ç­¾ï¼š")
for theme, label in business_labels.items():
    print(f"{theme}: {label}")