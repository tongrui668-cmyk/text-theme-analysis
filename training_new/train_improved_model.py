import pandas as pd
import numpy as np
import re
import jieba
import joblib
import os
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ“‹ 1. åŠ è½½æ•°æ®æ–‡ä»¶
file_path = '../data/raw/è¯„è®ºå’Œæ­£æ–‡.xlsx'
print(f"åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
df = pd.read_excel(file_path)

# æ£€æŸ¥æ˜¯å¦åŒ…å« 'è¯„è®ºå†…å®¹' åˆ—
if 'è¯„è®ºå†…å®¹' not in df.columns:
    raise ValueError("æ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘ 'è¯„è®ºå†…å®¹' åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ã€‚")

print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")

# ğŸ“‹ 2. æ–‡æœ¬é¢„å¤„ç†å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰
def clean_text(text):
    if pd.isnull(text):
        return ""
    
    # åŸºæœ¬æ¸…æ´—ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•°å­—
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
    
    # ä½¿ç”¨jiebaåˆ†è¯ï¼Œæ·»åŠ è‡ªå®šä¹‰è¯å…¸
    jieba.load_userdict('../training/custom_dict.txt')
    words = jieba.lcut(text)
    
    # è¿‡æ»¤æ‰å•å­—ç¬¦å’Œåœç”¨è¯
    stop_words = set(['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'])
    filtered_words = [word.strip() for word in words if len(word.strip()) > 1 and word not in stop_words]
    
    return " ".join(filtered_words)

# åº”ç”¨æ–‡æœ¬é¢„å¤„ç†
print("å¼€å§‹æ–‡æœ¬é¢„å¤„ç†...")
df['cleaned_text'] = df['è¯„è®ºå†…å®¹'].apply(clean_text)

# è¿‡æ»¤æ‰ç©ºçš„æ–‡æœ¬
df = df[df['cleaned_text'].str.len() > 0]
print(f"é¢„å¤„ç†å®Œæˆï¼Œè¿‡æ»¤åå‰©ä½™ {len(df)} æ¡æœ‰æ•ˆè®°å½•")

# ğŸ“‹ 3. å‘é‡åŒ–æ–‡æœ¬æ•°æ®ï¼ˆæ”¹è¿›å‚æ•°ï¼‰
print("å¼€å§‹å‘é‡åŒ–æ–‡æœ¬æ•°æ®...")
# ä½¿ç”¨æ›´åˆç†çš„å‚æ•°
vectorizer = CountVectorizer(
    max_features=8000,      # å¢åŠ ç‰¹å¾æ•°é‡
    min_df=2,               # è‡³å°‘åœ¨2ä¸ªæ–‡æ¡£ä¸­å‡ºç°
    max_df=0.9,             # æœ€å¤šåœ¨90%çš„æ–‡æ¡£ä¸­å‡ºç°
    ngram_range=(1, 2)      # åŒ…å«1-gramå’Œ2-gram
)
X = vectorizer.fit_transform(df['cleaned_text'])
print(f"å‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {X.shape[1]}")

# ğŸ“‹ 4. è®­ç»ƒLDAæ¨¡å‹ï¼ˆæ”¹è¿›å‚æ•°ï¼‰
print("å¼€å§‹è®­ç»ƒLDAæ¨¡å‹...")
n_topics = 5  # ä¿æŒ5ä¸ªä¸»é¢˜ï¼Œä¸ç°æœ‰ç³»ç»Ÿä¸€è‡´

# ä½¿ç”¨æ”¹è¿›çš„å‚æ•°
lda = LatentDirichletAllocation(
    n_components=n_topics,
    random_state=42,
    max_iter=50,           # å¢åŠ è¿­ä»£æ¬¡æ•°
    learning_method='online',  # ä½¿ç”¨åœ¨çº¿å­¦ä¹ æ–¹æ³•
    learning_offset=10.0,     # å­¦ä¹ ç‡åç§»
    doc_topic_prior=0.1,       # æ–‡æ¡£ä¸»é¢˜å…ˆéªŒ
    topic_word_prior=0.01      # ä¸»é¢˜è¯å…ˆéªŒ
)
lda.fit(X)
print("LDAæ¨¡å‹è®­ç»ƒå®Œæˆ")

# ğŸ“‹ 5. è¾“å‡ºæ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
def get_lda_keywords(model, vectorizer, n_top_words=15):
    keywords = {}
    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(model.components_):
        keywords[f'ä¸»é¢˜ {topic_idx + 1}'] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
    return keywords

# æå–å…³é”®è¯
keywords = get_lda_keywords(lda, vectorizer)
print("\nè‡ªåŠ¨ç”Ÿæˆçš„ä¸»é¢˜å…³é”®è¯ï¼š")
for theme, words in keywords.items():
    print(f"{theme}: {', '.join(words[:10])}")

# ğŸ“‹ 6. ä¸ºæ¯æ¡æ–‡æœ¬åˆ†é…ä¸»é¢˜
print("ä¸ºæ–‡æœ¬åˆ†é…ä¸»é¢˜...")
doc_topic_dist = lda.transform(X)
df['dominant_topic'] = doc_topic_dist.argmax(axis=1)

# ğŸ“‹ 7. å‡†å¤‡TF-IDFç‰¹å¾
print("åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨...")
tfidf_vectorizer = TfidfVectorizer(
    max_features=8000,
    min_df=2,
    max_df=0.9,
    ngram_range=(1, 2)
)
X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])
print(f"TF-IDFå‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {X_tfidf.shape[1]}")

# ğŸ“‹ 8. åˆå¹¶ç‰¹å¾
print("åˆå¹¶ç‰¹å¾...")
X_combined = np.hstack([doc_topic_dist, X_tfidf.toarray()])
print(f"åˆå¹¶åç‰¹å¾ç»´åº¦: {X_combined.shape[1]}")

# ğŸ“‹ 9. è®­ç»ƒä¸»é¢˜åˆ†ç±»å™¨ï¼ˆæ”¹è¿›å‚æ•°ï¼‰
print("å¼€å§‹è®­ç»ƒä¸»é¢˜åˆ†ç±»å™¨...")
X_train, X_test, y_train, y_test = train_test_split(
    X_combined, df['dominant_topic'], test_size=0.2, random_state=42
)

# ä½¿ç”¨æ”¹è¿›çš„RandomForestå‚æ•°
theme_classifier = RandomForestClassifier(
    n_estimators=100,        # å¢åŠ æ ‘çš„æ•°é‡
    max_depth=50,            # å¢åŠ æ ‘çš„æ·±åº¦
    min_samples_split=5,     # æœ€å°åˆ†è£‚æ ·æœ¬æ•°
    min_samples_leaf=2,      # æœ€å°å¶èŠ‚ç‚¹æ ·æœ¬æ•°
    random_state=42,
    n_jobs=-1                # ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒ
)
theme_classifier.fit(X_train, y_train)

# ğŸ“‹ 10. è¯„ä¼°æ¨¡å‹
print("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
y_pred = theme_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nä¸»é¢˜åˆ†ç±»å™¨å‡†ç¡®ç‡: {accuracy:.4f}")

# æ‰“å°è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=[f'ä¸»é¢˜ {i+1}' for i in range(n_topics)]))

# è®¡ç®—å›°æƒ‘åº¦
perplexity = lda.perplexity(X)
print(f"\nLDAæ¨¡å‹å›°æƒ‘åº¦: {perplexity:.2f}")

# ğŸ“‹ 11. ä¿å­˜æ¨¡å‹å’Œå‘é‡åŒ–å™¨
model_dir = '../data/models'
os.makedirs(model_dir, exist_ok=True)

print(f"\nä¿å­˜æ¨¡å‹åˆ° {model_dir}...")
joblib.dump(lda, os.path.join(model_dir, 'lda_model.pkl'))
joblib.dump(vectorizer, os.path.join(model_dir, 'count_vectorizer.pkl'))
joblib.dump(tfidf_vectorizer, os.path.join(model_dir, 'vectorizer.pkl'))
joblib.dump(theme_classifier, os.path.join(model_dir, 'theme_classification_model.pkl'))
joblib.dump(keywords, os.path.join(model_dir, 'theme_keywords.pkl'))

print("\næ‰€æœ‰æ¨¡å‹å·²ä¿å­˜å®Œæˆï¼")
print(f"- LDAæ¨¡å‹: lda_model.pkl")
print(f"- Countå‘é‡åŒ–å™¨: count_vectorizer.pkl")
print(f"- TF-IDFå‘é‡åŒ–å™¨: vectorizer.pkl")
print(f"- ä¸»é¢˜åˆ†ç±»å™¨: theme_classification_model.pkl")
print(f"- ä¸»é¢˜å…³é”®è¯: theme_keywords.pkl")

# ğŸ“‹ 12. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
print("\nè®­ç»ƒå®Œæˆï¼Œç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
with open('train_log_new.txt', 'w', encoding='utf-8') as f:
    f.write("=== æ¨¡å‹è®­ç»ƒæŠ¥å‘Š ===\n")
    f.write(f"è®­ç»ƒæ—¶é—´: {pd.Timestamp.now()}\n")
    f.write(f"è®­ç»ƒæ•°æ®é‡: {len(df)}\n")
    f.write(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}\n")
    f.write(f"åˆ†ç±»å™¨å‡†ç¡®ç‡: {accuracy:.4f}\n")
    f.write(f"LDAå›°æƒ‘åº¦: {perplexity:.2f}\n\n")
    f.write("ä¸»é¢˜å…³é”®è¯:\n")
    for theme, words in keywords.items():
        f.write(f"{theme}: {', '.join(words[:10])}\n")

print("è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: train_log_new.txt")