#!/usr/bin/env python3
# ä¸»é¢˜æ¸…æ™°åº¦ä¼˜åŒ–è„šæœ¬

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
from src.data_preprocessor import DataPreprocessor
from src.topic_modeler import TopicModeler
from src.text_preprocessor_enhanced import EnhancedTextPreprocessor


class TopicClarityOptimizer:
    """ä¸»é¢˜æ¸…æ™°åº¦ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.text_preprocessor = EnhancedTextPreprocessor()
        self.data_preprocessor = DataPreprocessor(self.text_preprocessor)
        self.topic_modeler = TopicModeler()
    
    def load_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
        df = self.data_preprocessor.load_data()
        df = self.data_preprocessor.preprocess_data(df)
        df_train, _, _ = self.data_preprocessor.split_data(df)
        return df_train
    
    def optimize_topic_clarity(self):
        """ä¼˜åŒ–ä¸»é¢˜æ¸…æ™°åº¦"""
        print("å¼€å§‹ä¼˜åŒ–ä¸»é¢˜æ¸…æ™°åº¦...")
        
        # åŠ è½½æ•°æ®
        df_train = self.load_data()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        vectorizer = self.topic_modeler.create_count_vectorizer()
        X_train_count = vectorizer.fit_transform(df_train['cleaned_text'])
        print(f"å‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {X_train_count.shape[1]}")
        
        # å®šä¹‰å‚æ•°ç½‘æ ¼
        param_grid = {
            'topic_word_prior': [0.0001, 0.001, 0.01, 0.1],
            'doc_topic_prior': [0.1, 0.3, 0.5],
            'max_iter': [500, 800, 1000],
            'n_topics': [5, 6, 7, 8]
        }
        
        best_clarity = 0.0
        best_params = {}
        
        # éå†å‚æ•°ç»„åˆ
        total_combinations = (
            len(param_grid['topic_word_prior']) *
            len(param_grid['doc_topic_prior']) *
            len(param_grid['max_iter']) *
            len(param_grid['n_topics'])
        )
        print(f"æ€»å‚æ•°ç»„åˆæ•°: {total_combinations}")
        
        current = 0
        for topic_word_prior in param_grid['topic_word_prior']:
            for doc_topic_prior in param_grid['doc_topic_prior']:
                for max_iter in param_grid['max_iter']:
                    for n_topics in param_grid['n_topics']:
                        current += 1
                        print(f"\næµ‹è¯•å‚æ•°ç»„åˆ {current}/{total_combinations}:")
                        print(f"topic_word_prior={topic_word_prior}, doc_topic_prior={doc_topic_prior}, max_iter={max_iter}, n_topics={n_topics}")
                        
                        # åˆ›å»ºå¹¶è®­ç»ƒLDAæ¨¡å‹
                        from sklearn.decomposition import LatentDirichletAllocation
                        from src.config import config
                        
                        lda = LatentDirichletAllocation(
                            n_components=n_topics,
                            random_state=config.LDA['random_state'],
                            max_iter=max_iter,
                            learning_method=config.LDA['learning_method'],
                            learning_offset=config.LDA['learning_offset'],
                            doc_topic_prior=doc_topic_prior,
                            topic_word_prior=topic_word_prior
                        )
                        
                        try:
                            lda.fit(X_train_count)
                            
                            # è®¡ç®—ä¸»é¢˜æ¸…æ™°åº¦
                            topic_entropy = []
                            feature_names = vectorizer.get_feature_names_out()
                            for topic in lda.components_:
                                topic_dist = topic / topic.sum()
                                entropy = -np.sum(topic_dist * np.log(topic_dist + 1e-10))
                                topic_entropy.append(entropy)
                            avg_topic_clarity = 1 - (np.mean(topic_entropy) / np.log(len(feature_names)))
                            
                            # è®¡ç®—å›°æƒ‘åº¦
                            perplexity = lda.perplexity(X_train_count)
                            
                            print(f"  ä¸»é¢˜æ¸…æ™°åº¦: {avg_topic_clarity:.4f}, å›°æƒ‘åº¦: {perplexity:.2f}")
                            
                            # æ›´æ–°æœ€ä½³å‚æ•°
                            if avg_topic_clarity > best_clarity:
                                best_clarity = avg_topic_clarity
                                best_params = {
                                    'topic_word_prior': topic_word_prior,
                                    'doc_topic_prior': doc_topic_prior,
                                    'max_iter': max_iter,
                                    'n_topics': n_topics,
                                    'clarity': avg_topic_clarity,
                                    'perplexity': perplexity
                                }
                                print(f"  ğŸ” å‘ç°æ›´ä½³å‚æ•°ç»„åˆ!")
                                
                        except Exception as e:
                            print(f"  âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
                            continue
        
        # è¾“å‡ºæœ€ä½³å‚æ•°
        print("\n" + "="*60)
        print("æœ€ä½³å‚æ•°ç»„åˆ:")
        print("="*60)
        for key, value in best_params.items():
            print(f"{key}: {value}")
        print("="*60)
        
        return best_params


if __name__ == "__main__":
    optimizer = TopicClarityOptimizer()
    best_params = optimizer.optimize_topic_clarity()
    print("\nä¼˜åŒ–å®Œæˆ!")