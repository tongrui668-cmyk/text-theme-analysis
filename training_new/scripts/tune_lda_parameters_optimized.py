#!/usr/bin/env python3
# LDAå‚æ•°è‡ªåŠ¨è°ƒä¼˜è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

import sys
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import config
from src.topic_modeler import TopicModeler
from src.data_preprocessor import DataPreprocessor
from src.text_preprocessor import TextPreprocessor

class LDAParameterTuner:
    """LDAå‚æ•°è‡ªåŠ¨è°ƒä¼˜å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‚æ•°è°ƒä¼˜å™¨"""
        # å®šä¹‰è¦æµ‹è¯•çš„å‚æ•°èŒƒå›´ï¼ˆåŸºäºåˆæ­¥æµ‹è¯•ç»“æœï¼Œåªæµ‹è¯•æœ€æœ‰å¸Œæœ›çš„ç»„åˆï¼‰
        self.parameter_grid = {
            'max_iter': [500, 800, 1000],
            'doc_topic_prior': [0.1],
            'topic_word_prior': [0.0001, 0.001],
            'learning_offset': [15.0]
        }
        
        # åŠ è½½æ•°æ®
        self.data = self._load_data()
    
    def _load_data(self):
        """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
        print("åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®...")
        text_processor = TextPreprocessor()
        processor = DataPreprocessor(text_processor)
        df = processor.load_data()
        df = processor.preprocess_data(df)
        return df['cleaned_text'].tolist()
    
    def tune_parameters(self):
        """
        è‡ªåŠ¨è°ƒä¼˜LDAå‚æ•°
        
        Returns:
            dict: æœ€ä½³å‚æ•°ç»„åˆå’Œå¯¹åº”çš„æ€§èƒ½æŒ‡æ ‡
        """
        print("å¼€å§‹è‡ªåŠ¨è°ƒä¼˜LDAå‚æ•°...")
        
        best_params = None
        best_perplexity = float('inf')
        best_coherence = 0
        
        total_combinations = np.prod([len(v) for v in self.parameter_grid.values()])
        print(f"æ€»å…±éœ€è¦æµ‹è¯• {total_combinations} ç§å‚æ•°ç»„åˆ...")
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        from itertools import product
        param_combinations = list(product(
            self.parameter_grid['max_iter'],
            self.parameter_grid['doc_topic_prior'],
            self.parameter_grid['topic_word_prior'],
            self.parameter_grid['learning_offset']
        ))
        
        # æµ‹è¯•æ¯ä¸ªå‚æ•°ç»„åˆ
        for i, (max_iter, doc_topic_prior, topic_word_prior, learning_offset) in enumerate(param_combinations):
            print(f"\næµ‹è¯•å‚æ•°ç»„åˆ {i+1}/{total_combinations}:")
            print(f"max_iter={max_iter}, doc_topic_prior={doc_topic_prior}, topic_word_prior={topic_word_prior}, learning_offset={learning_offset}")
            
            # åˆ›å»ºå¹¶é…ç½®TopicModeler
            modeler = TopicModeler()
            
            # ä¸´æ—¶ä¿®æ”¹é…ç½®
            original_config = config.LDA.copy()
            config.LDA['max_iter'] = max_iter
            config.LDA['doc_topic_prior'] = doc_topic_prior
            config.LDA['topic_word_prior'] = topic_word_prior
            config.LDA['learning_offset'] = learning_offset
            
            try:
                # è¯„ä¼°æœ€ä½³ä¸»é¢˜æ•°
                best_n_topics = modeler.evaluate_topic_numbers(self.data)
                
                # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
                modeler.train_final_model(self.data, best_n_topics)
                
                # è¯„ä¼°æ€§èƒ½
                perplexity = modeler.evaluate_perplexity()
                coherence = modeler.evaluate_coherence()
                
                print(f"  å›°æƒ‘åº¦: {perplexity:.2f}, ä¸€è‡´æ€§: {coherence:.3f}, æœ€ä½³ä¸»é¢˜æ•°: {best_n_topics}")
                
                # æ›´æ–°æœ€ä½³å‚æ•°
                if perplexity < best_perplexity:
                    best_perplexity = perplexity
                    best_coherence = coherence
                    best_params = {
                        'max_iter': max_iter,
                        'doc_topic_prior': doc_topic_prior,
                        'topic_word_prior': topic_word_prior,
                        'learning_offset': learning_offset,
                        'n_topics': best_n_topics
                    }
                    print(f"  ğŸ¯ æ‰¾åˆ°æ–°çš„æœ€ä½³å‚æ•°ç»„åˆ!")
                    
            except Exception as e:
                print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            finally:
                # æ¢å¤åŸå§‹é…ç½®
                config.LDA = original_config
        
        print("\n" + "="*60)
        print("å‚æ•°è°ƒä¼˜å®Œæˆ!")
        print(f"æœ€ä½³å‚æ•°ç»„åˆ: {best_params}")
        print(f"æœ€ä½å›°æƒ‘åº¦: {best_perplexity:.2f}")
        print(f"æœ€ä½³ä¸€è‡´æ€§: {best_coherence:.3f}")
        print("="*60)
        
        return {
            'best_params': best_params,
            'best_perplexity': best_perplexity,
            'best_coherence': best_coherence
        }

def main():
    """ä¸»å‡½æ•°"""
    tuner = LDAParameterTuner()
    result = tuner.tune_parameters()
    
    # ä¿å­˜è°ƒä¼˜ç»“æœ
    import json
    result_path = Path(__file__).parent.parent / 'tuning_results.json'
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nè°ƒä¼˜ç»“æœå·²ä¿å­˜è‡³: {result_path}")

if __name__ == "__main__":
    main()
