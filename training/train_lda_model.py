import pandas as pd
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
import jieba
import joblib
import os

# ğŸ“‹ 1. åŠ è½½æ•°æ®æ–‡ä»¶
file_path = '../data/raw/è¯„è®ºå’Œæ­£æ–‡.xlsx'  # æ•°æ®æ–‡ä»¶è·¯å¾„
df = pd.read_excel(file_path)

# æ£€æŸ¥æ˜¯å¦åŒ…å« 'è¯„è®ºå†…å®¹' åˆ—
if 'è¯„è®ºå†…å®¹' not in df.columns:
    raise ValueError("æ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘ 'è¯„è®ºå†…å®¹' åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ã€‚")

# ğŸ“‹ 2. æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def clean_text(text):
    if pd.isnull(text):
        return ""
    words = jieba.lcut(text)
    return " ".join(words)

# åº”ç”¨æ–‡æœ¬é¢„å¤„ç†
df['cleaned_text'] = df['è¯„è®ºå†…å®¹'].apply(clean_text)

# ğŸ“‹ 3. å‘é‡åŒ–æ–‡æœ¬æ•°æ®
vectorizer = CountVectorizer(max_features=5000, stop_words='english')
X = vectorizer.fit_transform(df['cleaned_text'])

# ğŸ“‹ 4. è®­ç»ƒ LDA æ¨¡å‹
n_topics = 5  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ä¸»é¢˜æ•°é‡
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(X)

# ğŸ“‹ 5. è¾“å‡ºæ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
def get_lda_keywords(model, vectorizer, n_top_words=10):
    keywords = {}
    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(model.components_):
        keywords[f'ä¸»é¢˜ {topic_idx + 1}'] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
    return keywords

# æå–å…³é”®è¯
keywords = get_lda_keywords(lda, vectorizer)
print("\nè‡ªåŠ¨ç”Ÿæˆçš„ä¸»é¢˜å…³é”®è¯ï¼š")
for theme, words in keywords.items():
    print(f"{theme}: {', '.join(words)}")

# ğŸ“‹ 6. ä¿å­˜æ¨¡å‹å’Œå‘é‡åŒ–å™¨
model_dir = '../data/models'
os.makedirs(model_dir, exist_ok=True)
joblib.dump(lda, os.path.join(model_dir, 'lda_model.pkl'))
joblib.dump(vectorizer, os.path.join(model_dir, 'vectorizer.pkl'))
print("\nLDA æ¨¡å‹å’Œå‘é‡åŒ–å™¨å·²ä¿å­˜åˆ° data/models/ ç›®å½•ï¼")
