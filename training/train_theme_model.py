import pandas as pd
import numpy as np
import re
import thulac  # æ›¿æ¢ jieba ä¸º thulac
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import joblib
import os
import seaborn as sns
import matplotlib.pyplot as plt

# åˆå§‹åŒ–åˆ†è¯å™¨
thu = thulac.thulac(seg_only=True)

# ğŸ“‹ 1. åŠ è½½æ•°æ®æ–‡ä»¶
file_path = '../data/raw/è¯„è®ºå’Œæ­£æ–‡.xlsx'
if not os.path.exists(file_path):
    file_path = 'data/raw/è¯„è®ºå’Œæ­£æ–‡.xlsx'
df = pd.read_excel(file_path)

# æ£€æŸ¥æ•°æ®åˆ—æ˜¯å¦å­˜åœ¨
if 'è¯„è®ºå†…å®¹' not in df.columns:
    raise ValueError("æ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘ 'è¯„è®ºå†…å®¹' åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ã€‚")

# ğŸ“‹ 2. æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯
def clean_text(text):
    if not isinstance(text, str):
        return ''
    # åŸºæœ¬æ¸…æ´—ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•°å­—
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
    # THULACåˆ†è¯
    words = thu.cut(text, text=True)
    # è¿‡æ»¤æ‰å•å­—ç¬¦å’Œåœç”¨è¯
    filtered_words = [word.strip() for word in words.split() if len(word.strip()) > 1]
    return ' '.join(filtered_words)

df['cleaned_text'] = df['è¯„è®ºå†…å®¹'].apply(clean_text)
print(f"é¢„å¤„ç†åæ–‡æœ¬æ ·æœ¬: {df['cleaned_text'].iloc[0] if len(df) > 0 else 'None'}")
print(f"éç©ºæ–‡æœ¬æ•°é‡: {df['cleaned_text'].str.len().gt(0).sum()}")

# è¿‡æ»¤æ‰ç©ºçš„æ–‡æœ¬
df = df[df['cleaned_text'].str.len() > 0]
print(f"è¿‡æ»¤åæ•°æ®é‡: {len(df)}")

# ğŸ“‹ 3. åŠ¨æ€è°ƒæ•´ä¸»é¢˜æ•°é‡
def find_optimal_topics(X, start=2, end=5):
    print("å¼€å§‹å¯»æ‰¾æœ€ä½³ä¸»é¢˜æ•°...")
    perplexities = []
    for n_topics in range(start, end + 1):
        print(f"æµ‹è¯•ä¸»é¢˜æ•°: {n_topics}")
        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)
        lda.fit(X)
        perplexity = lda.perplexity(X)
        perplexities.append((n_topics, perplexity))
        print(f"ä¸»é¢˜æ•° {n_topics}: å›°æƒ‘åº¦ = {perplexity:.2f}")
    optimal_topics = min(perplexities, key=lambda x: x[1])[0]
    return optimal_topics

# å‘é‡åŒ–æ•°æ®
print("å¼€å§‹å‘é‡åŒ–æ•°æ®...")
vectorizer = CountVectorizer(max_features=5000, min_df=1, max_df=0.95)
X = vectorizer.fit_transform(df['cleaned_text'])
print(f"Countå‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {X.shape[1]}")

# åŠ¨æ€ç¡®å®šæœ€ä½³ä¸»é¢˜æ•°
print("ä½¿ç”¨å›ºå®šä¸»é¢˜æ•°è¿›è¡Œè®­ç»ƒ...")
n_topics = 5  # ç›´æ¥ä½¿ç”¨å›ºå®šä¸»é¢˜æ•°
print(f"ç¡®å®šä¸»é¢˜æ•°: {n_topics}")
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=20)
lda.fit(X)
print("LDAæ¨¡å‹è®­ç»ƒå®Œæˆ")

# ğŸ“‹ 4. æå–ä¸»é¢˜å…³é”®è¯
def get_lda_keywords(model, vectorizer, n_top_words=10):
    keywords = {}
    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(model.components_):
        keywords[f'ä¸»é¢˜ {topic_idx + 1}'] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
    return keywords

theme_keywords = get_lda_keywords(lda, vectorizer)

# æ‰“å°ä¸»é¢˜å…³é”®è¯
print("\nè‡ªåŠ¨ç”Ÿæˆçš„ä¸»é¢˜å…³é”®è¯ï¼š")
for theme, words in theme_keywords.items():
    print(f"{theme}: {', '.join(words)}")

# ğŸ“‹ 5. è®­ç»ƒä¸»é¢˜åˆ†ç±»å™¨
print("å¼€å§‹è®­ç»ƒä¸»é¢˜åˆ†ç±»å™¨...")
# ä¸ºæ¯æ¡æ–‡æœ¬åˆ†é…ä¸»è¦ä¸»é¢˜
doc_topic_dist = lda.transform(X)
df['dominant_topic'] = doc_topic_dist.argmax(axis=1)

# å‡†å¤‡è®­ç»ƒæ•°æ®ï¼šåˆå¹¶LDAç‰¹å¾å’ŒTF-IDFç‰¹å¾
from sklearn.feature_extraction.text import TfidfVectorizer

# åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
print("åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨...")
tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=1, max_df=0.95)
X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])
print(f"TF-IDFå‘é‡åŒ–å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {X_tfidf.shape[1]}")

# åˆå¹¶ç‰¹å¾
print("åˆå¹¶ç‰¹å¾...")
X_combined = np.hstack([doc_topic_dist, X_tfidf.toarray()])
print(f"åˆå¹¶åç‰¹å¾ç»´åº¦: {X_combined.shape[1]}")

# è®­ç»ƒä¸»é¢˜åˆ†ç±»å™¨
print("è®­ç»ƒRandomForeståˆ†ç±»å™¨...")
X_train, X_test, y_train, y_test = train_test_split(
    X_combined, df['dominant_topic'], test_size=0.2, random_state=42
)

theme_classifier = RandomForestClassifier(n_estimators=10, random_state=42, n_jobs=1)
theme_classifier.fit(X_train, y_train)

# è¯„ä¼°æ¨¡å‹
y_pred = theme_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"\nä¸»é¢˜åˆ†ç±»å™¨å‡†ç¡®ç‡: {accuracy:.4f}")

# ğŸ“‹ 6. ä¿å­˜æ‰€æœ‰æ¨¡å‹å’Œå‘é‡åŒ–å™¨
models_dir = '../data/models'
os.makedirs(models_dir, exist_ok=True)
joblib.dump(lda, os.path.join(models_dir, 'lda_model.pkl'))
joblib.dump(vectorizer, os.path.join(models_dir, 'count_vectorizer.pkl'))  # CountVectorizer
joblib.dump(tfidf_vectorizer, os.path.join(models_dir, 'vectorizer.pkl'))  # TF-IDF Vectorizer
joblib.dump(theme_classifier, os.path.join(models_dir, 'theme_classification_model.pkl'))
joblib.dump(theme_keywords, os.path.join(models_dir, 'theme_keywords.pkl'))

print(f"\næ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ° {models_dir}")
print(f"- LDAæ¨¡å‹: lda_model.pkl")
print(f"- Countå‘é‡åŒ–å™¨: count_vectorizer.pkl") 
print(f"- TF-IDFå‘é‡åŒ–å™¨: vectorizer.pkl")
print(f"- ä¸»é¢˜åˆ†ç±»å™¨: theme_classification_model.pkl")
print(f"- ä¸»é¢˜å…³é”®è¯: theme_keywords.pkl")
